{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "coll_points = np.load(\"./data/sampling_points.npz\")[\"coll_points\"]\n",
    "bc1_points = np.load(\"./data/sampling_points.npz\")[\"bc1_points\"]\n",
    "bc2_points = np.load(\"./data/sampling_points.npz\")[\"bc2_points\"]\n",
    "bc3_points = np.load(\"./data/sampling_points.npz\")[\"bc3_points\"]\n",
    "bc4_points = np.load(\"./data/sampling_points.npz\")[\"bc4_points\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "coll_points = jnp.array(coll_points)\n",
    "bc1_points = jnp.array(bc1_points)\n",
    "bc2_points = jnp.array(bc2_points)\n",
    "bc3_points = jnp.array(bc3_points)\n",
    "bc4_points = jnp.array(bc4_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training start...\n",
      "Epoch 00100: Total Loss = 4.11e+00\n",
      "Epoch 00200: Total Loss = 1.06e+00\n",
      "Epoch 00300: Total Loss = 7.75e-01\n",
      "Epoch 00400: Total Loss = 6.91e-01\n",
      "Epoch 00500: Total Loss = 6.54e-01\n",
      "Epoch 00600: Total Loss = 6.30e-01\n",
      "Epoch 00700: Total Loss = 6.10e-01\n",
      "Epoch 00800: Total Loss = 5.91e-01\n",
      "Epoch 00900: Total Loss = 5.72e-01\n",
      "Epoch 01000: Total Loss = 5.52e-01\n",
      "Epoch 01100: Total Loss = 5.33e-01\n",
      "Epoch 01200: Total Loss = 5.14e-01\n",
      "Epoch 01300: Total Loss = 4.96e-01\n",
      "Epoch 01400: Total Loss = 4.80e-01\n",
      "Epoch 01500: Total Loss = 4.66e-01\n",
      "Epoch 01600: Total Loss = 4.54e-01\n",
      "Epoch 01700: Total Loss = 4.43e-01\n",
      "Epoch 01800: Total Loss = 4.33e-01\n",
      "Epoch 01900: Total Loss = 4.25e-01\n",
      "Epoch 02000: Total Loss = 4.17e-01\n",
      "Epoch 02100: Total Loss = 4.09e-01\n",
      "Epoch 02200: Total Loss = 4.02e-01\n",
      "Epoch 02300: Total Loss = 3.96e-01\n",
      "Epoch 02400: Total Loss = 3.90e-01\n",
      "Epoch 02500: Total Loss = 3.85e-01\n",
      "Epoch 02600: Total Loss = 3.80e-01\n",
      "Epoch 02700: Total Loss = 3.75e-01\n",
      "Epoch 02800: Total Loss = 3.71e-01\n",
      "Epoch 02900: Total Loss = 3.66e-01\n",
      "Epoch 03000: Total Loss = 3.63e-01\n",
      "Epoch 03100: Total Loss = 3.59e-01\n",
      "Epoch 03200: Total Loss = 3.55e-01\n",
      "Epoch 03300: Total Loss = 3.52e-01\n",
      "Epoch 03400: Total Loss = 3.49e-01\n",
      "Epoch 03500: Total Loss = 3.46e-01\n",
      "Epoch 03600: Total Loss = 3.43e-01\n",
      "Epoch 03700: Total Loss = 3.40e-01\n",
      "Epoch 03800: Total Loss = 3.37e-01\n",
      "Epoch 03900: Total Loss = 3.35e-01\n",
      "Epoch 04000: Total Loss = 3.32e-01\n",
      "Epoch 04100: Total Loss = 3.30e-01\n",
      "Epoch 04200: Total Loss = 3.27e-01\n",
      "Epoch 04300: Total Loss = 3.25e-01\n",
      "Epoch 04400: Total Loss = 3.23e-01\n",
      "Epoch 04500: Total Loss = 3.21e-01\n",
      "Epoch 04600: Total Loss = 3.19e-01\n",
      "Epoch 04700: Total Loss = 3.17e-01\n",
      "Epoch 04800: Total Loss = 3.16e-01\n",
      "Epoch 04900: Total Loss = 3.14e-01\n",
      "Epoch 05000: Total Loss = 3.12e-01\n",
      "Epoch 05100: Total Loss = 3.10e-01\n",
      "Epoch 05200: Total Loss = 3.08e-01\n",
      "Epoch 05300: Total Loss = 3.06e-01\n",
      "Epoch 05400: Total Loss = 3.03e-01\n",
      "Epoch 05500: Total Loss = 3.01e-01\n",
      "Epoch 05600: Total Loss = 2.99e-01\n",
      "Epoch 05700: Total Loss = 2.96e-01\n",
      "Epoch 05800: Total Loss = 2.92e-01\n",
      "Epoch 05900: Total Loss = 2.88e-01\n",
      "Epoch 06000: Total Loss = 2.83e-01\n",
      "Epoch 06100: Total Loss = 2.77e-01\n",
      "Epoch 06200: Total Loss = 2.71e-01\n",
      "Epoch 06300: Total Loss = 2.64e-01\n",
      "Epoch 06400: Total Loss = 2.56e-01\n",
      "Epoch 06500: Total Loss = 2.46e-01\n",
      "Epoch 06600: Total Loss = 2.35e-01\n",
      "Epoch 06700: Total Loss = 2.23e-01\n",
      "Epoch 06800: Total Loss = 2.11e-01\n",
      "Epoch 06900: Total Loss = 2.00e-01\n",
      "Epoch 07000: Total Loss = 1.92e-01\n",
      "Epoch 07100: Total Loss = 1.85e-01\n",
      "Epoch 07200: Total Loss = 1.78e-01\n",
      "Epoch 07300: Total Loss = 1.72e-01\n",
      "Epoch 07400: Total Loss = 1.67e-01\n",
      "Epoch 07500: Total Loss = 1.61e-01\n",
      "Epoch 07600: Total Loss = 1.56e-01\n",
      "Epoch 07700: Total Loss = 1.52e-01\n",
      "Epoch 07800: Total Loss = 1.48e-01\n",
      "Epoch 07900: Total Loss = 1.44e-01\n",
      "Epoch 08000: Total Loss = 1.41e-01\n",
      "Epoch 08100: Total Loss = 1.38e-01\n",
      "Epoch 08200: Total Loss = 1.35e-01\n",
      "Epoch 08300: Total Loss = 1.32e-01\n",
      "Epoch 08400: Total Loss = 1.29e-01\n",
      "Epoch 08500: Total Loss = 1.26e-01\n",
      "Epoch 08600: Total Loss = 1.24e-01\n",
      "Epoch 08700: Total Loss = 1.21e-01\n",
      "Epoch 08800: Total Loss = 1.19e-01\n",
      "Epoch 08900: Total Loss = 1.17e-01\n",
      "Epoch 09000: Total Loss = 1.14e-01\n",
      "Epoch 09100: Total Loss = 1.12e-01\n",
      "Epoch 09200: Total Loss = 1.09e-01\n",
      "Epoch 09300: Total Loss = 1.06e-01\n",
      "Epoch 09400: Total Loss = 1.02e-01\n",
      "Epoch 09500: Total Loss = 9.89e-02\n",
      "Epoch 09600: Total Loss = 9.63e-02\n",
      "Epoch 09700: Total Loss = 9.43e-02\n",
      "Epoch 09800: Total Loss = 9.28e-02\n",
      "Epoch 09900: Total Loss = 9.16e-02\n",
      "Epoch 10000: Total Loss = 9.05e-02\n",
      "Epoch 10100: Total Loss = 8.97e-02\n",
      "Epoch 10200: Total Loss = 8.90e-02\n",
      "Epoch 10300: Total Loss = 8.83e-02\n",
      "Epoch 10400: Total Loss = 8.77e-02\n",
      "Epoch 10500: Total Loss = 8.72e-02\n",
      "Epoch 10600: Total Loss = 8.68e-02\n",
      "Epoch 10700: Total Loss = 8.64e-02\n",
      "Epoch 10800: Total Loss = 8.61e-02\n",
      "Epoch 10900: Total Loss = 8.58e-02\n",
      "Epoch 11000: Total Loss = 8.55e-02\n",
      "Epoch 11100: Total Loss = 8.53e-02\n",
      "Epoch 11200: Total Loss = 8.51e-02\n",
      "Epoch 11300: Total Loss = 8.49e-02\n",
      "Epoch 11400: Total Loss = 8.47e-02\n",
      "Epoch 11500: Total Loss = 8.45e-02\n",
      "Epoch 11600: Total Loss = 8.43e-02\n",
      "Epoch 11700: Total Loss = 8.42e-02\n",
      "Epoch 11800: Total Loss = 8.40e-02\n",
      "Epoch 11900: Total Loss = 8.39e-02\n",
      "Epoch 12000: Total Loss = 8.37e-02\n",
      "Epoch 12100: Total Loss = 8.36e-02\n",
      "Epoch 12200: Total Loss = 8.35e-02\n",
      "Epoch 12300: Total Loss = 8.34e-02\n",
      "Epoch 12400: Total Loss = 8.33e-02\n",
      "Epoch 12500: Total Loss = 8.31e-02\n",
      "Epoch 12600: Total Loss = 8.30e-02\n",
      "Epoch 12700: Total Loss = 8.29e-02\n",
      "Epoch 12800: Total Loss = 8.28e-02\n",
      "Epoch 12900: Total Loss = 8.27e-02\n",
      "Epoch 13000: Total Loss = 8.26e-02\n",
      "Epoch 13100: Total Loss = 8.25e-02\n",
      "Epoch 13200: Total Loss = 8.24e-02\n",
      "Epoch 13300: Total Loss = 8.24e-02\n",
      "Epoch 13400: Total Loss = 8.23e-02\n",
      "Epoch 13500: Total Loss = 8.22e-02\n",
      "Epoch 13600: Total Loss = 8.21e-02\n",
      "Epoch 13700: Total Loss = 8.20e-02\n",
      "Epoch 13800: Total Loss = 8.19e-02\n",
      "Epoch 13900: Total Loss = 8.18e-02\n",
      "Epoch 14000: Total Loss = 8.17e-02\n",
      "Epoch 14100: Total Loss = 8.16e-02\n",
      "Epoch 14200: Total Loss = 8.15e-02\n",
      "Epoch 14300: Total Loss = 8.15e-02\n",
      "Epoch 14400: Total Loss = 8.13e-02\n",
      "Epoch 14500: Total Loss = 8.12e-02\n",
      "Epoch 14600: Total Loss = 8.11e-02\n",
      "Epoch 14700: Total Loss = 8.10e-02\n",
      "Epoch 14800: Total Loss = 8.09e-02\n",
      "Epoch 14900: Total Loss = 8.08e-02\n",
      "Epoch 15000: Total Loss = 8.07e-02\n",
      "Epoch 15100: Total Loss = 8.06e-02\n",
      "Epoch 15200: Total Loss = 8.05e-02\n",
      "Epoch 15300: Total Loss = 8.04e-02\n",
      "Epoch 15400: Total Loss = 8.03e-02\n",
      "Epoch 15500: Total Loss = 8.02e-02\n",
      "Epoch 15600: Total Loss = 8.01e-02\n",
      "Epoch 15700: Total Loss = 8.00e-02\n",
      "Epoch 15800: Total Loss = 7.99e-02\n",
      "Epoch 15900: Total Loss = 7.98e-02\n",
      "Epoch 16000: Total Loss = 7.97e-02\n",
      "Epoch 16100: Total Loss = 7.96e-02\n",
      "Epoch 16200: Total Loss = 7.96e-02\n",
      "Epoch 16300: Total Loss = 7.95e-02\n",
      "Epoch 16400: Total Loss = 7.94e-02\n",
      "Epoch 16500: Total Loss = 7.93e-02\n",
      "Epoch 16600: Total Loss = 7.92e-02\n",
      "Epoch 16700: Total Loss = 7.91e-02\n",
      "Epoch 16800: Total Loss = 7.90e-02\n",
      "Epoch 16900: Total Loss = 7.90e-02\n",
      "Epoch 17000: Total Loss = 7.89e-02\n",
      "Epoch 17100: Total Loss = 7.88e-02\n",
      "Epoch 17200: Total Loss = 7.87e-02\n",
      "Epoch 17300: Total Loss = 7.87e-02\n",
      "Epoch 17400: Total Loss = 7.86e-02\n",
      "Epoch 17500: Total Loss = 7.85e-02\n",
      "Epoch 17600: Total Loss = 7.84e-02\n",
      "Epoch 17700: Total Loss = 7.85e-02\n",
      "Epoch 17800: Total Loss = 7.83e-02\n",
      "Epoch 17900: Total Loss = 7.82e-02\n",
      "Epoch 18000: Total Loss = 7.82e-02\n",
      "Epoch 18100: Total Loss = 7.81e-02\n",
      "Epoch 18200: Total Loss = 7.80e-02\n",
      "Epoch 18300: Total Loss = 7.80e-02\n",
      "Epoch 18400: Total Loss = 7.79e-02\n",
      "Epoch 18500: Total Loss = 7.79e-02\n",
      "Epoch 18600: Total Loss = 7.78e-02\n",
      "Epoch 18700: Total Loss = 7.77e-02\n",
      "Epoch 18800: Total Loss = 7.76e-02\n",
      "Epoch 18900: Total Loss = 7.76e-02\n",
      "Epoch 19000: Total Loss = 7.75e-02\n",
      "Epoch 19100: Total Loss = 7.74e-02\n",
      "Epoch 19200: Total Loss = 7.74e-02\n",
      "Epoch 19300: Total Loss = 7.73e-02\n",
      "Epoch 19400: Total Loss = 7.73e-02\n",
      "Epoch 19500: Total Loss = 7.72e-02\n",
      "Epoch 19600: Total Loss = 7.72e-02\n",
      "Epoch 19700: Total Loss = 7.71e-02\n",
      "Epoch 19800: Total Loss = 7.70e-02\n",
      "Epoch 19900: Total Loss = 7.70e-02\n",
      "Epoch 20000: Total Loss = 7.69e-02\n",
      "Training complete!\n",
      "\n",
      "Total training time: 6.74 seconds\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random, jit, grad, value_and_grad\n",
    "import optax\n",
    "from functools import partial\n",
    "import time\n",
    "\n",
    "# ------------------------------\n",
    "# 모델 정의 (이미 작성한 mlp_jax 사용)\n",
    "import mlp_jax\n",
    "\n",
    "key = random.PRNGKey(42)\n",
    "layer_sizes = [2, 5, 5, 1]\n",
    "params = mlp_jax.init_mlp_params(layer_sizes, key)\n",
    "\n",
    "# ------------------------------\n",
    "# Loss 함수 정의 (PINN 예제)\n",
    "\n",
    "# PDE Loss 예시: (Laplace PDE)\n",
    "def pde_residual(params, x):\n",
    "    u = lambda x: mlp_jax.mlp_forward(params, x, activation=\"tanh\").squeeze()\n",
    "    grad_u = jax.grad(u)\n",
    "    grad_u_x = lambda x: grad_u(x)[0]\n",
    "    grad_u_y = lambda x: grad_u(x)[1]\n",
    "\n",
    "    u_xx = jax.grad(grad_u_x)\n",
    "    u_yy = jax.grad(grad_u_y)\n",
    "\n",
    "    return u_xx(x)[0] + u_yy(x)[1]\n",
    "\n",
    "# Losses (각 조건에 맞게 수정 필요)\n",
    "@jit\n",
    "def total_loss(params, coll_points, bc1, bc2, bc3, bc4):\n",
    "    pde_loss = jnp.mean(jax.vmap(lambda x: pde_residual(params, x)**2)(coll_points))\n",
    "    bc1_loss = jnp.mean((mlp_jax.mlp_forward(params, bc1) - 0)**2)\n",
    "    bc2_loss = jnp.mean((mlp_jax.mlp_forward(params, bc2) - 1)**2)\n",
    "    bc3_loss = jnp.mean((mlp_jax.mlp_forward(params, bc3) - 0)**2)\n",
    "    bc4_loss = jnp.mean((mlp_jax.mlp_forward(params, bc4) - 0)**2)\n",
    "\n",
    "    return pde_loss + bc1_loss + bc2_loss + bc3_loss + bc4_loss\n",
    "\n",
    "# ------------------------------\n",
    "# 최적화 설정 (Optax 사용 권장)\n",
    "\n",
    "learning_rate = 1e-3\n",
    "optimizer = optax.adam(learning_rate)\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "# ------------------------------\n",
    "# 학습 루프 정의 (JIT 컴파일)\n",
    "\n",
    "@jit\n",
    "def train_step(params, opt_state, coll_points, bc1, bc2, bc3, bc4):\n",
    "    loss_val, grads = value_and_grad(total_loss)(params, coll_points, bc1, bc2, bc3, bc4)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state, loss_val\n",
    "\n",
    "# ------------------------------\n",
    "# Training 실행 루프\n",
    "\n",
    "epochs = 20000\n",
    "loss_history = []\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"Training start...\")\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    params, opt_state, loss_val = train_step(params, opt_state, coll_points, bc1_points, bc2_points, bc3_points, bc4_points)\n",
    "    \n",
    "    loss_history.append(loss_val.item())\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch:05d}: Total Loss = {loss_val:.2e}\")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Training complete!\\n\")\n",
    "print(f\"Total training time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# ------------------------------\n",
    "# 결과 저장 (numpy를 사용하여 저장)\n",
    "import numpy as np\n",
    "np.save(\"./results/loss_data_jax.npy\", np.array(loss_history))\n",
    "\n",
    "# 모델 파라미터 저장\n",
    "import pickle\n",
    "with open(\"./results/model_jax.pkl\", \"wb\") as f:\n",
    "    pickle.dump(params, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax-0.4.38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
